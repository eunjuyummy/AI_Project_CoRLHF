# 6주차에 이은 실험 방법 : 개입 방식의 차이
## • 기존 방식 : invervene가 참일 때 학습된 정책(policy_action)과 전문가 행동(expert_action)을 50% 섞어 행동을 결정 하도록 함
## • 개선 방식 : 학습된 정책(policy_action)의 비율을 0.7로 변경

# 실험 결과 
### 0.7로 Policy action의 비율을 높였을 때 0.5때와는 반대로 EAE가 더 높은 성능을 보여줍니다.
### EAE(고편향, 저분산 데이터)는 항상 최적의 행동을 취하려고 하기 때문에 유연하게 반응하는 policy action을 활용하여 정책이 이 행동을 모방하게 되면 성능이 좋아집니다.
### 0.5에서 EAE는 충분히 학습되지 않은 상태(policy action이 낮을때)에서는 오히려 성능을 저해하는 결과를 보여줍니다. 
### Medium 데이터셋은 다양한 리워드의 탐색을 포함하고 있어 이미 최적의 행동인 expert action이 높을 때 좋은 결과를 보여줍니다.
![image](https://github.com/user-attachments/assets/598f5ed0-95fd-48a0-bd2f-2f31cfae5cb7)

# 다음 실험 방법 제안 1. 점진적 개입 방식 (Progressive Intervention)
### 초기 학습 단계에서는 ratio가 1에 가까워서 policy action을 더 많이 따르다가, 시간이 지남에 따라 expert action의 비중이 커지는 방식
![image](https://github.com/user-attachments/assets/aeea5963-2646-4ea8-8b70-405f454eb9ad)

# 다음 실험 방법 제안 2. 역량 기반 개입 방식 (Competence-Based Intervention)
### 초기에는 expert action에 의존해 안정적인 학습을 유도하고, 성과가 개선될수록 policy action의 비율을 늘리는 방식
![image](https://github.com/user-attachments/assets/2cd5462b-816c-449e-a437-00e81dee8a33)







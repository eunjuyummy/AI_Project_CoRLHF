# 5강 Model Free Control
## 지난주 복습
MDP를 알고 있을 때 최적의 Policy를 찾는 방법(Model Based): DP
MDP를 모를 때 최적의 Policy를 찾는 방법(Model Free): MC, TD

- DP: 직접 가지 않고도 모든 MDP를 알고 있기 때문에 행동을 예상하고 belman 방정식을 이용하여 가장 큰 V(t)를 찾는다 return을 극대화, policy 업데이트를 반복하며 최적의 값을 찾는다.
- MC: 많은 경우의 수 중 샘플링을 통해 직접 경험하며 값을 업데이트 한다. 한 에피소드가 끝나야 값을 업데이트 할 수 있으며 값의 편차가 크고 dept가 크다.
- TD: 추측으로 추측을 업데이트하는 방식으로 에피소드가 끝나지 않아도 업데이트가 가능하다.
단순히 보기에는 MC보다 TD가 좋아 보이지만 실제로 문제를 해결하다보면 꼭 그렇지만은 않다.

## On-Policy와 Off-Policy
Policy는 현재 내가 학습하고 있는 Behavior policy와 최적의 Policy를 찾기위해 업데이트 하는 Target Policy가 있다.
이 둘이 일치할 시 On-Policy, 다르면 Off-Policy라고 한다.

Off-Policy는 Agent가 직접 경험하지 않고 다른 Agent가 경험한 내용을 가지고 업데이트 할 수 있으며 human expert로 부터 만들어진 데이터도 학습에 적용할 수 있어 강력한 학습 방법이라고 할 수 있다.

## E-Greedy Exploration
문이 2개가 있다. 왼쪽 문을 열었을 때 reward가 0이고 오른쪽문을 열었을 때 reward가 1이 주어졌다. 다시 오른쪽 문을 열었을 때 reward + 2가 주어진다면 Agent는 계속해서 오른쪽 문만을 열 것이다.
그렇다면 그 다음번째에 왼쪽문을 열었을 때 더 좋은 리워드를 얻을 수 없다고 확신할 수 있을까? 

항상가는 식당에서 맛있게 먹는 메뉴가 있지만 다른 메뉴가 더 맛있을 수도 있다.
이처럼 때로는 최적의 방법 외에도 탐험을 통해 다른 방법도 시도해보는 것이 도움이 될 때가 있다

이 떄 필요한 것이 엡실론 그리디이다. 
엡실론 그리디가 0.05라면 Agent는 1-0.05만큼 최적의 행동을 수행하며 0.05%의 확률로 다른 행동을 수행한다. 그리고 많은 시도를 거치며 엡실론 그리디의 값을 축소시켜 작은 값으로 수렴시킨다.


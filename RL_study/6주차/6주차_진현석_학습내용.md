# Lecture 5 

## Model-Free Control을 사용하는 경우
- 1. MDP model이 알려져 있지 않으나(환경을 모름) 경험은 샘플링 가능할 때
- 2. MDP model이 알려져 있으나 너무 커서 샘플만 사용할 수밖에 없을 때

## On-policy
- π에서 샘플링된 경험에서 π를 업데이트
## Off-policy
- μ(행동 정책, behaviour policy)에서 샘플링된 경험에서 π를 업데이트

## V(s)는 greedy policy불가
- Model-free에서는 V(s)로  Policy evaluation는 가능하지만  Policy improvement는 불가능 why?
- V(s)를 바탕으로 greedy policy를 한다는 것은 다음 state를 안다는 것 = MDP를 안다 But Model-free에서는 MDP를 모른다.
- 따라서 Q(s,a) 사용

## Q(s,a)이란?
- 특정 상태 s에서 특정 행동 a를 선택했을 때 얻을 수 있는 예상 미래 보상의 총합
- Q(s,a)로 greedy policy 구할 수 있다.

## Sarsa란?
- state S에서 action A를 해 reward R을 받고 state S'에 가 action A'를 한다. 그 즉시 value function을 업데이트



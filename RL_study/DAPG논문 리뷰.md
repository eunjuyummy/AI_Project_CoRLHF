# DAPG의 구성 요소
 ## Demonstration Learning(시범학습) : 
- 초기 정책을 설정하는 데 사용
- expert data (즉, 전문가의 행동을 기록한 데이터)를 이용하여 초기 정책을 학습
- 초기 탐색 단계를 빠르게 진행

 ## Policy Gradient RL: 
- 시범 학습을 통해 초기화된 정책을 기반으로, 강화 학습을 통해 정책을 Fine-tuning하는 방식
- 실제 과제 목표를 최적화

 ## 시범을 통한 학습 신호 보강: 
- 학습 과정 중 시범 데이터를 활용하여 보조 학습 신호를 제공
- 정책이 시범 데이터와 크게 벗어나지 않도록 유도

##  DAPG와 NPG 비교

## 초기화 단계: 
### 1.DAPG
- Behavior Cloning을 사용하여 전문가 시범을 모방
- 초기 탐색 과정을 단축
### 2. NPG:
- 강화 학습만으로 학습
- 초기 탐색 단계에서 많은 샘플이 필요, 학습 속도 감소

## 3.샘플 복잡도: 
 ### 1.DAPG
- 모방으로 샘플 복잡도를 크게 줄임
- DAPG가 NPG보다 최대 30배 더 빠르게 학습 가능
### 2. NPG
- 강화 학습만으로 학습하기 때문에 더 많은 샘플이 필요
- 이로 인해 학습 시간 증가

## 성능 및 안정성: 
### 1.DAPG
- Behavior Cloning 을 통해 초기 정책을 설정한 후, 강화 학습을 통해 개선
- 이 과정에서 정책의 안정성과 성능 증가
### 2. NPG
- 샘플 복잡도 문제와 초기 탐색 단계에서의 어려움
- DAPG에 비해 성능, 안정성 감소

## 결론

- 논문에서는 DAPG가 NPG보다 학습 효율성과 성능 면에서 우수하다는 것을 다양한 실험을 통해 입증
- Expert data 를 통해 초기 Policy을 설정하고, 이를 강화 학습을 통해 성능을 향상시키는 
- DAPG는 특히 초기 탐색 단계에서의 어려움을 극복하는 데 유리하며, 샘플 복잡도를 크게 감소
- DAPG는 복잡한 손재주 조작 과제에서 NPG보다 더  나은 성과를 보여줌




Lecture 3: Planning by Dynamic Programming

-Policy Iteration
: policy를 두고 이를 업데이트해나간다.
-Policy iteration의 두 단계
i) policy evaluation: 새로운 policy π에 대한 vπ를 계산한다.
ii) policy improvement: vπ에 기반해 π′ <= π인 π′를 만들어 새로운 policy로 삼는다.
-Value Iteration
: policy 없이 value function 만을 optimal value function이 되도록 수정해 나간다.
-Async DP
: state의 백업을 각각, 임의의 순서로 진행한다.
-In-Place DP
: value array의 copy를 생성해 swap하는 게 아니라, 하나의 value array에 state를 계속 업데이트하는 방식이다.
-Real-Time DP
: agent가 방문하는 state들부터 업데이트한다.

Lecture 4: Model-Free Prediction

-Monte-Carlo(MC)
: agent가 π에 따라 episode들을 진행하도록 만든 후 그 결과에 따라 vπ​(s)를 평가
-Temporal-Difference Learning(TD)
: 현재 시점 t에서 상태 St​의 가치 V(St​)를 바로 다음 시점 t+1에 수정하는 방식
-Bias-Variance Trade-Off
-Bootstrapping
: 업데이트가 추정을 포함한다.(MC X, DP O, TD O)
-Sampling
: 업데이트가 샘플을 이용한다.(MC O, DP X, TD O)
-online
: value function을 episode 시행 도중 업데이트한다.
-offline
: episode가 모두 끝나고 시행한다.

RLHF 논문 리뷰

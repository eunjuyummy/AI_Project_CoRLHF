# 5주차 학습내용
## RL-3
## RL-4
## RLHF (Deep Reinforcement Learning from Human Preferences) 논문 리뷰
### 1. Introduction
#### (1-1). RL의 단점 
- 강화학습(RL)을 대규모 문제로 확장하는 데 성공한 사례는 보상함수가 잘 정의된 영역에서 이루어짐.
- 많은 task는 복잡하거나, 제대로 정의되지 않았거나, 구체화하기 어려운 목표를 포함하고 있음.
- 의도한 행동을 포착하는 간단한 보상함수를 설계할 수도 있음 → 보상을 최적화하는 행동으로 이어질 수 있음(reward hacking).
- 실제 목표를 에이전트에게 성공적으로 전달할 수 있다면, 실제 가치와 RL 시스템의 목표 사이의 불일치 문제를 해결할 수 있음.
#### (1-2). RL의 단점을 해결하기 위한 방법
- Inverse Reinforcement Learning
- BC, DAgger
#### (1-3). 잘 정의된 보상함수 없이 순차적 의사결정 문제를 해결하기 위한 조건
1. 원하는 동작을 인식만 할 수 있으면 됨, 반드시 시연할 필요는 없음.
2. 비전문가인 사용자도 에이전트를 교육할 수 있음.
3. 대규모 문제까지 확장 가능함.
4. 사용자 피드백을 통해 경제적.
<img width="250" alt="image" src="https://github.com/eunjuyummy/AI_Project_CoRLHF/assets/101487529/5b7b2712-78c1-440a-b02a-ed49f756880c">

### 2. Preliminaries and Method
#### (2-1). Setting and Goal
- 기존 RL의 경우 환경이 reward signal을 agent에게 제공함.
- 본 논문의 경우, trajectory segment 사이의 선호도를 표현할 수 있는 인간이 있다고 가정함.
- trajectory segments: σ=((o0,a0),(o1,a1),..(ok−1,ak−1))∈(O×A)
- The comparisons are modeled as being generated from a Bradley-Terry (or Boltzmann rational) model, where the probability of preferring trajectory A over B is proportional to the exponential of the difference between the return of trajectory A minus B
- 알고리즘에 대한 행동을 평가하는 두 가지 방법 (Quantitative, Qualitative)
#### (2-2). Our Method
#### (2-3). Optimizing the Policy
- 해당 domain에 적합한 RL 알고리즘을 사용하여 문제를 해결
#### (2-4). Preference Elicitaiton
#### (2-5). Fitting the Reward Function
#### (2-6). Selecting Queries

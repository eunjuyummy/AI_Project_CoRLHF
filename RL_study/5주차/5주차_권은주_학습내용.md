# 5주차 학습내용
## RL-3. Planning by Dynamic Programming
### Introduction
- Dynamic programming은 MDP에 대한 모든 지식을 알고 있음.
- planning은 MDP에 대한 full knowledge가 있을 때 최적의 policy를 찾는 방법.
- prediction: value function을 찾는 방법
- control: optimal한 value function or policy를 찾는 방법
### Policy Evaluation (prediction)
- policy를 따랐을 때의 value function을 찾는 문제
- policy를 평가만 해서, 평가된 value 안에서 greedy하게 움직이면 더 나은 policy를 찾을 수 있음.
### Policy Iteration (control)
- Evaluate를 하고 greedy를 하면 improve된 policy를 찾을 수 있음. (policy evaluation과 policy improvement의 반복)
- Policy Improvement (1): 무조건 이전 policy 보다 좋은가? -> Yes! 증명
  - Deterministic: 과거의 원인이 미래의 결과가 되는 것 
- Policy Improvement (2)
  - 정책이 더 이상 개선되지 않을 때, 모든 상태에 s에 대해 정책의 가치함수와 최적가치함수가 같다면 정책pi는 최적의 pi*이 됨.
- Modified Policy Iteration: K번만 하고 evaluation, improve해도 됨.
- Iterative policy evaluation, Greedy policy improvement 대신 어떠한 policy evaluation algorithm, policy improvement algorithm을 사용해도 됨.
### value Iteration (control)
- Principle of Optimality
- 최적 정책을 따를 때, 현재 상태에서 취하는 첫 번째 행동과 그 이후에 취하는 모든 행동들도 최적의 행동이어야 함. 한 상태에서의 최적의 행동은 그 상태에 이르기까지의 결정들과 독립적이어야 함.
- (Theorem):
  - 어떤 정책 π(a∣s)가 상태 s에서 최적의 가치를 달성한다, 즉 vπ(s) = v∗(s)
  - 이는 다음과 같은 조건에서만 참: s에서 도달 가능한 모든 상태 s'에 대해
  - 정책 π는 s'에서의 최적의 가치를 달성해야 함. vπ(s') = v∗(s')
- Deterministic Value Iteration
  - subproblems v*(s')의 솔루션을 알면 Bellman Opimality Equation을 통해 s에서의 solution을 구할 수 있음.
  - 명시적인 policy 없이 value만 가지고 업데이트
## RL-4. Model Free Prediction

## RLHF (Deep Reinforcement Learning from Human Preferences) 논문 리뷰
### 1. Introduction
#### (1-1). RL의 단점 
- 강화학습(RL)을 대규모 문제로 확장하는 데 성공한 사례는 보상함수가 잘 정의된 영역에서 이루어짐.
- 많은 task는 복잡하거나, 제대로 정의되지 않았거나, 구체화하기 어려운 목표를 포함하고 있음.
- 의도한 행동을 포착하는 간단한 보상함수를 설계할 수도 있음 → 보상을 최적화하는 행동으로 이어질 수 있음(reward hacking).
- 실제 목표를 에이전트에게 성공적으로 전달할 수 있다면, 실제 가치와 RL 시스템의 목표 사이의 불일치 문제를 해결할 수 있음.
#### (1-2). RL의 단점을 해결하기 위한 방법
- Inverse Reinforcement Learning
- BC, DAgger
#### (1-3). 잘 정의된 보상함수 없이 순차적 의사결정 문제를 해결하기 위한 조건
1. 원하는 동작을 인식만 할 수 있으면 됨, 반드시 시연할 필요는 없음.
2. 비전문가인 사용자도 에이전트를 교육할 수 있음.
3. 대규모 문제까지 확장 가능함.
4. 사용자 피드백을 통해 경제적.
<img width="250" alt="image" src="https://github.com/eunjuyummy/AI_Project_CoRLHF/assets/101487529/5b7b2712-78c1-440a-b02a-ed49f756880c">

### 2. Preliminaries and Method
#### (2-1). Setting and Goal
- 기존 RL의 경우 환경이 reward signal을 agent에게 제공함.
- 본 논문의 경우, trajectory segment 사이의 선호도를 표현할 수 있는 인간이 있다고 가정함.
- trajectory segments: σ=((o0,a0),(o1,a1),..(ok−1,ak−1))∈(O×A)
- The comparisons are modeled as being generated from a Bradley-Terry (or Boltzmann rational) model, where the probability of preferring trajectory A over B is proportional to the exponential of the difference between the return of trajectory A minus B
- 알고리즘에 대한 행동을 평가하는 두 가지 방법 (Quantitative, Qualitative)
#### (2-2). Our Method
- Deep neural Network의 parameter를 통해 reward function r^: O×A→R를 추정함.
- Policy를 통해서 Trajectory를 생성하고 해당 Trajectory가 받은 feedback을 통해 reward function을 수정함.
- networks 업데이트 과정
  - policy π를 통해 agent는 환경과 상호작용하여 trajectory 집합 [ τ1,τ2,…,τi] 를 생성
  - trajectory 집합으로부터 segments (σ1,σ2)을 선택하고 인간에게 feedback을 받음.
  - feedback을 target으로 supervised learning.즉, r^의 parameter를 feedback에 매핑
- 모든 과정은 asynchronously 이루어짐. -> 동시에 진행될 필요 없음.
#### (2-3). Optimizing the Policy
- 해당 domain에 적합한 RL 알고리즘을 사용하여 문제를 해결
#### (2-4). Preference Elicitaiton
- 인간 평가자에게 2가지의 trajectory segments (σk,σk−1)을 제공함.
- 인간이 선호도를 제공함.
- μ: {1,2} 로 구성된 분포로 사람의 선호도를 나타내는 변수 -> 인간 피드백에 따라 μ의 값이 달라짐.
- 인간이 판단 불가능하다고 선택한 경우, 해당되는 data는 집합 D에 포함되지 않음.
#### (2-5). Fitting the Reward Function
- predictions와 사람의 feedback 사이의 Cross-Entropy loss가 최소로 되게 하는 r^을 선택
- L(r^)=−∑μ(1)logP^[σ1>σ2]+μ(2)logP[σ2>σ1]
#### (2-6). Selecting Queries
- reward function estimator의 불확실성에 대한 근사치를 기반으로 선호도를 쿼리하는 방법을 결정
- 길이 k의 segment를 샘플링하고 앙상블의 각 보상 예측자를 사용하여 각 쌍에서 선호되는 segment를 예측한 다음 예측값이 가장 높은 segment를 선택함.

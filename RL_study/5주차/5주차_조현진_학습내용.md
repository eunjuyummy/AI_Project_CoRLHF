# 3강 Planning by Dynamic Programming

prediction = value function을 update하는 과정

control = optimal policy를 가지고 action을 고르는 것
두 가지가 순차적으로 번갈아가며 학습이 진행 됨

policy iteration = 초기 policy를 설정하고 평가하고 개선하는 과정을 반복하며 최적의 policy를 찾는 것 
policy evalueation -> greedy한 policy -> policy eval...
반복 될 수록 *최적으로 수렴
Bellman Optimalilty Equation을 사용하여 value function을 update

# 4강 Model Free Prediction

## Monte Carlo(MC)

에이전트가 env에서 에피소드를 끝까지 수행한 수 그 에피소드의 결과를 바탕으로 학습하는 방식
- 시작에서 출발하여 종료 상태에 도달할 때까지의 일련의 상태, 행동, 보상 시퀀스
- 업데이트 방식: 에피소드를 끝까지 진행 후 실제 총 보상을 계산하여 value function을 업데이트

단순하고 직관적이나 에피소드가 끝나기 전까지 업데이트를 할 수 없다

## Temporal Difference (TD)

에이전트가 step마다 현재 상태와 다음 상태의 가치 차이를 이용해 학습하는 방식

- 업데이트 방식: step이 끝날 때마다 현재 상태의 가치함수가 다음 상태의 가치 함수와 보상을 바탕으로 즉시 업데이트

매 단계별로 빠른 업데이트가 가능하나 노이즈가 포함될 확률이 있다.

두 방법 중 어느 것이 더 좋다라고 말하기 힘들며 두 방법 모두 off policy의 강력한 방법이다.

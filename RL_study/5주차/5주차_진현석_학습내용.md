# Lecture 3: Planning by Dynamic Programming

- Policy Iteration
: policy를 두고 이를 업데이트해나간다.
- Policy iteration의 두 단계
i) policy evaluation: 새로운 policy π에 대한 vπ를 계산한다.
ii) policy improvement: vπ에 기반해 π′ <= π인 π′를 만들어 새로운 policy로 삼는다.
- Value Iteration
: policy 없이 value function 만을 optimal value function이 되도록 수정해 나간다.
- Async DP
: state의 백업을 각각, 임의의 순서로 진행한다.
- In-Place DP
: value array의 copy를 생성해 swap하는 게 아니라, 하나의 value array에 state를 계속 업데이트하는 방식이다.
- Real-Time DP
: agent가 방문하는 state들부터 업데이트한다.

# Lecture 4: Model-Free Prediction

- Monte-Carlo(MC)
: agent가 π에 따라 episode들을 진행하도록 만든 후 그 결과에 따라 vπ​(s)를 평가
- Temporal-Difference Learning(TD)
: 현재 시점 t에서 상태 St​의 가치 V(St​)를 바로 다음 시점 t+1에 수정하는 방식
- Bias-Variance Trade-Off
- Bootstrapping
: 업데이트가 추정을 포함한다.(MC X, DP O, TD O)
- Sampling
: 업데이트가 샘플을 이용한다.(MC O, DP X, TD O)
- online
: value function을 episode 시행 도중 업데이트한다.
- offline
: episode가 모두 끝나고 시행한다.

# RLHF 논문 리뷰
-RLHF의 접근방식
1.보상 함수에 접근할 필요 없이 복잡한 RL 과제를 효과적으로 해결할 수 있다.
2.환경과의 상호 작용 중 1% 미만에 대한 피드백을 제공함으로써 인간 감독의 비용을 충분히 줄여 실제로 최신 RL 시스템에 적용한다.
3.인간의 시간 약 한 시간으로 복잡한 새로운 행동을 성공적으로 훈련시킬 수 있음을 보여준다.
-RLHF의 알고리즘 
1.인간의 선호도에 맞춰 보상 함수를 조정하는 동시에 현재 예측된 보상 함수를 최적화하기 위한 정책을 훈련한다.
2.인간에게 에이전트의 행동에 대한 짧은 비디오 클립을 비교하도록 요청한다.
-RLHF의 목표
-RL을 능가하는 것이 아니라 보상 정보에 접근하지 않고 훨씬 더 드문 피드백에 의존하면서 거의 비슷하게 잘 수행하는 것.

RLHF 논문 느낀점
말하고자 하는 바는 전체적으로 알 수 있었지만 RLIF 논문과 마찬가지로 실험을 하는데에 있어 전문적인 식 전개를 이해하기는 어려웠다.

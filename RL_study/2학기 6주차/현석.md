# 제안했던 실험 방법 : 개입 방식의 차이
## 기존 방식 : 기존에는 ‘label’ 전략을 사용하지 않기 때문에 개입이 일어난 경우 expert_action만 사용되는 구조
## 개선 방식: 조건을 제거하고 invervene가 참일 때 학습된 정책(policy_action)과 전문가 행동(expert_action)을 50% 섞어 	행동을 결정 하도록 함
![image](https://github.com/user-attachments/assets/c2df14fe-e02a-4a56-836b-84ed41761d64)
## 방식 선정 이유 
기존의＂label＂전략은 전문가의 행동을 완전히 따르기 때문에, 학습된 정책이 사용되지 않는다.
Medium 데이터 => 저편향 • 고분산
즉, 개입이 발생했을때 두 정책의 평균값을 사용하여 좀 더 유연한 학습을 진행시키고자함

# 실험 결과
## Medium 데이터가 기존의 Expert 데이터 보다 더 높은 성능을 보여준다
### 학습의 다양성과 유연성은 Medium 데이터와 상호 보완적 관계이다
### 소요 시간: 모델 당 약 27~28시간

![image](https://github.com/user-attachments/assets/17ccacd9-a872-4cfa-83a6-6ed0cd52c887)

# 다음 실험 제안 
### 110EA-E와 110EA-M을 동일한 방식으로 학습해 40EA와 비교
### 1번의 결과에 따라 학습의 다양성과 유연성을 더 강조할지 여부 결정



lecture_5: 수업을 이해하기 위한 난이도가 높았다. 

RLIF 논문리뷰: RLIF란 강화학습과 대화식 모방 학습을 결합한 새로운 학습방법이다.
전문가의 개입 보상신호를 활용하여 에이전트를 학습시키는 방법으로 옵티멀한 보상 함수 없이도 전문가의 개입을 통해
에이전트가 올바른 행동을 학습할 수 있게 한다.

논문은 RLIF의 성능을 입증하고 기존 방법보다 뛰어난 성능을 보인다는 것을 알려주지만
정확한 방법과 코드에 대한 해석에는 미치지 못했다.

lecture1 ~ 2: 
기본적으로 강화학습을 배우기 위해 정리해야 할 개념이 몇가지 있다.
agent는 행동을 하는 주체로서 게임에서의 캐릭터에 비유할 수 있다. 이 agent는 action을 취하고 그 대가로 agent를 둘러싼 env로부터
reward(+/-)와 observation을 받는다. 

시점1에서부터 t까지 O1,R1,A1를 한 묶음으로 Ot, Rt, At까지 합친 것을 Ht(History)라고 하는데
이것은 다음에 무엇이 일어날지 결정하는 state를 결정하는 함수의 인자로 들어가게 된다.

즉 스테이트란 다음 시점에 무엇이 일어날지 결정하는 모든 요소이다.

Markov state는 강화학습에 중요한 개념 중 하나이다.
t시점의 State에서 t+1시점의 State가 될 확률은 S1에서 St까지의 State를 고려한 것과 같다.

즉 미래에 일어날 State t+1은 현재 St에 의존하고 S1~St-1까지의 과거에 독립적이다.
바로 직전의 현재만으로 미래를 예측할 수 있다는 말이 된다.

Policy 는 Agent 행동의 근거로써 표현된다. State란 인자가 주어졌을 때 Agent가 어떻게 행동해야할 지의 지침이 되며 파이로 표현한다.

Value Function(return)은 Reward와 다른 개념으로 어떤 시점 St로부터 어떤 Policy 파이를 따랐을 때 미래 t+1부터 쭈욱 얻을 수 있는 Reward의 합을 Return이라 한다.

Model은 env이 어떻게 될 지 예측하는 것으로
Model Free와 Model Based로 구분된다. 이후 강의에서 자세하게 설명된다.

Markov Reward Process
Markov Reward Process는 튜플로 <S, P>를 나타낸 것.








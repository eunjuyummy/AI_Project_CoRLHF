# RLIF 코드 흐름도 및 관계 재정립
## 코드 실행 과정
### 개입 여부를 판단 후 개입 시 expert_action을 개입 X시 policy_action을 행동 
### 각 action에 따라 상태 전이(state transition)발생하고, 이때 reward도 얻게 됨
### 이 보상 정보와 Dateset이 적절한 비율로 합쳐져 Replay Buffer를 생성
![image](https://github.com/user-attachments/assets/d8c57184-069c-45eb-9041-18e9e051f79a)

## EA, Dataset, Action이란
### EA(110EA, 40EA)
110EA: 전문가 대비 110% 성능의 에이전트
40EA: 전문가 대비 40% 성능의 에이전트

### Action(expert_action, policy_action)
expert_action: 개입 시 전문가의 행동
policy_action: 개입 X시 agent의 행동

### Dataset(medium dataset, expert dataset)
medium dataset: 저편향, 고분산의 데이터셋
expert dataset: 고편향, 저분산의 데이터셋
![image](https://github.com/user-attachments/assets/47b648e8-865a-4ce3-9d58-dd4316870865)

## EA – Dataset 관계
### Expert 데이터셋 특징
고편향 (High Bias): 전문가의 최적 행동을 바탕으로 한 데이터, 특정 정책이나 행동에 강하게 편향
저분산 (Low Variance): 보상은 일관되게 높은 보상을 얻기 위한 행동에 집중되어 있어, 보상의 분포가 비교적 좁고 일정
=> 탐험 없이 높은 성과를 지속적으로 달성하나, 데이터의 다양성이 부족하여 여러 상황에 대한 적응력이 떨어짐

### Medium 데이터 특징
저편향 (Low Bias): 다양한 행동과 상태를 포함하고 있어 특정 정책이나 행동에 편향되지 않은 데이터를 제공
고분산 (High Variance): 보상은 여러 가지 상태와 행동에 걸쳐 넓게 분포되어 있으며, 에이전트가 다양한 상황에서 얻은 보상을 포함하고 있음을 의미
=> 다양한 상황에 대한 예시를 포함하여 에이전트가 여러 시나리오에 적응하고 넓은 범위의 문제를 해결하도록 도움

## Action – Dataset 관계
기존의 결과에 비해 개입 시 Expert_action과 Policy_action의 비율을 조절하는 것은 성능 변화의 영향이 거의 없다는 것을 보여줌
Why? Dataset의 다양성과 특성이 에이전트의 성능에 더 큰 영향을 주기 때문
![image](https://github.com/user-attachments/assets/0bf6a204-1ad9-446b-8c7b-e5f6a797d2f9)



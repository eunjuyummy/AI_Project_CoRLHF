# RLPD란?
## RLPD의 등장 배경: 기존 강화 학습의 문제점 
많은 환경과의 상호작용을 필요로 하며, 이는 실세계 문제에서 비용과 시간 증가
보상이 희소하거나 상태와 행동 공간이 고차원인 경우 학습의 어려움

## RLPD란?
기존의 복잡한 사전 학습이나 명시적인 모방 제약 X
온라인 RL과 오프라인 데이터를 결합하여 학습하는 강화 학습 알고리즘

## RLPD알고리즘 장점
간단한 설계: 기존 오프라인 RL 알고리즘 보다 단순한 설계를 통해 높은 성능
계산 효율성 유지: 추가적인 계산 오버헤드 없이 기존 온라인 RL의 장점을 유지하면서 오프라인 데이터를 통합
실세계 문제에 적용 가능: 복잡한 환경이나 데이터 수집 비용이 높은 실제 문제에 적용 기대  ex) 로봇 제어, 드론 제어 등

# RLPD 알고리즘 과정
![image](https://github.com/user-attachments/assets/f1e0bf4c-8c9f-4014-a05b-d836e7ed3ac9)
## Algorithm 1 Online RL with Offline Data (RLPD)
1. Initialization: LayerNorm, Ensemble size, Gradient steps 등 주요 파라미터 설정. Critic과 Actor 네트워크 초기화
2. Action 수행 및 데이터 저장: 현재 상태에서 Actor의 정책에 따라 행동을 선택하고, 결과(State, Action, Reward, Next state)를 Replay buffer에 저장
3. Batch Sampling: Replay buffer와 Offline data buffer에서 각각 50%씩 샘플링하여 학습 배치 구성
4. Critic Update: 손실을 최소화하도록 Critic 네트워크 업데이트
5. Actor Update: Q-value를 최대화하고 엔트로피를 추가하여 Actor 정책 업데이트	

# RLPD는 이전의 알고리즘 대비 최대 2.5배 높은 성능 향상
AntMaze, Sparse Adroit 등 여러 벤치마크에서 RLPD가 강력한 성능을 보임
AntMaze에서는 특히 이전 알고리즘보다 빠르게 목표를 달성하며, Sparse Adroit의 "Door" 작업에서는 2.5배 더 나은 성과를 기록
![image](https://github.com/user-attachments/assets/697207ac-f856-4dd0-ac58-b938e5757b72)





# 1.Behavioral Cloning (행위 복제, BC)

## 정의
- Expert의 행동을 직접 모방하는 모방학습의 기본적인 형태 
- Expert가 수행한 행동을 데이터로 수집하여 State-Action 쌍을 학습하고, 이를 통해 정책을 유도

## 특징
- 학습 데이터: State-Action 쌍
- 정책 학습: State에 대해 적절한 행동을 예측하도록 학습

## 장점
- 간단함: 알고리즘이 단순하고 구현이 쉬움
- 빠른 학습: Expert-Data가 충분하면 빠르게 학습
- 명확한 목표: Expert의 행동을 그대로 모방하는 명확한 목표를 보유

## 단점
- 데이터 의존성: Expert-Data 가 충분하지 않거나 불완전할 경우 성능이 저하될 우려
- 오차 증폭 문제: 학습된 정책이 전문가 데이터와 다른 행동을 할 경우, 그 차이가 누적되어 큰 오차로 이어짐
- 일반화 어려움: 새로운 상태에 대한 일반화가 어려울 수 있으며, 학습하지 않은 상태에서의 성능이 보장 X

# 2.Inverse Reinforcement Learning (역강화학습, IRL)

## 정의
- Expert의 행동을 통해 Reward 함수를 추정하고, 이를 기반으로 정책을 학습하는 방법 
- IRL은 Expert가 어떤 Reward 함수를 최대화하려고 하는지를 학습하는 것이 목표

## 특징
- 학습 데이터: Expert의 State-Action 궤적
- 정책 학습: 추정된 Reward 함수를 통해 최적의 정책을 학습

## 장점
- 보상 함수 학습: Reward 함수를 직접 학습하기 때문에, 학습된 정책이 보상 함수를 최대화하도록 유도
- 일반화 능력: 학습된 Reward 함수를 사용하여 다양한 환경에서 일반화된 정책 도출
- 효율적 학습: Expert-Data가 적을 때도 비교적 효율적으로 학습

## 단점
- 복잡성: Reward 함수를 추정하는 과정이 복잡하고 계산 비용이 많이 들 수 있다
- 불확실성: 추정된 Reward 함수를 Expert의 실제 의도를 정확히 반영하지 못할 수 있다
- 데이터 요구: Expert의 행동 궤적 데이터가 필요하며, 부정확하거나 불완전할 경우 성능에 영향을 미칠 수 있다

# 3. Generative Adversarial Imitation Learning (적대적 모방학습)

## 정의
- 생성적 적대 신경망(GAN)의 아이디어를 모방학습에 적용
- 두 개의 신경망, Generator(실제 데이터와 유사한 가짜 데이터를 생성하는 모델)와 Discriminator(주어진 데이터가 진짜인지 가짜인지를 판별하는 모델)가 서로 경쟁하면서 학습
- GAIL에서는 Generator가 Expert의 행동을 모방하고, Discriminator가 Expert의 행동과 Generator의 행동을 구별하며 학습

## 특징
- 학습 데이터: Expert의 State-Action 궤적
- 정책 학습: Generator가 Expert와 구분되지 않도록 학습

## 장점
- 표현력: Generator는 Expert의 복잡한 행동 패턴을 학습할 수 있는 강력한 모델을 사용
- 일반화 능력: 다양한 상태에서도 Expert와 유사한 행동을 생성할 수 있는 능력을 학습
- 자동 구별 학습: 구분자를 통해 Expert와 생성자의 행동을 자동으로 구별하며 학습

## 단점
- 학습 불안정성: GAN과 마찬가지로, 생성자와 구분자의 학습이 불안정할 수 있으며, 서로 균형을 맞추는 것이 어려울 수 있다
- 복잡성: 두 개의 네트워크를 동시에 학습시키는 것은 구현이 복잡하고 계산 비용이 많이 든다
- 훈련 시간: 생성자와 구분자 간의 상호 학습 과정이 시간이 오래 걸릴 수 있다

# 4.Dataset Aggregation (DAgger)

## 정의
- Expert의 시연 데이터와 Agent의 경험을 반복적으로 결합하여 학습하는 모방학습 방법
- 초기엔 Expert의 정책을 따라가다가, Agent가 수행한 행동을 추가로 수집하여 데이터셋을 확장하고, 점점 더 나은 정책을 학습

## 특징
- 학습 데이터: Expert의 State-Action 쌍과 Agent의 Action-Data를 결합
- 정책 학습: Expert 와 Agent의 혼합 데이터를 통해 점진적으로 개선되는 정책을 학습

## 장점
- 오차 축소: 전문가와 에이전트의 행동 데이터를 결합함으로써 오차 증폭 문제를 감소
- 향상된 성능: 반복적인 데이터셋 확장을 통해 에이전트의 정책이 점진적으로 개선
- 데이터 효율성: 전문가 데이터만으로 학습하는 것보다 더 적은 데이터로도 효율적으로 학습

## 단점
- 복잡성 증가: 반복적인 데이터셋 확장 과정이 추가적인 복잡성을 가져올 수 있다
- 전문가 개입 필요: 학습 과정 중 Expert의 개입이 필요할 수 있어, Expert의 노력이 많이 요구
- 훈련 시간 증가: 반복적인 데이터 수집과 학습 과정이 시간을 많이 소요









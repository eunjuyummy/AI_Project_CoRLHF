# 오프라인 강화학습

강화학습은 에이전트가 환경과 상호작용하며 보상을 최대화하는 행동을 학습하는 기계 학습이다. 
그러나 전통적인 온라인 강화학습 방법은 현실 세계의 문제에 직접적으로 적용하기에 대표적으로 다움과 같은 어려움 등이 있다. 

안전성: 에이전트가 학습 과정에서 잘못된 행동을 취했을 때 치명적인 결과를 초래할 수 있음
  예를 들어, 자율 주행 자동차나 의료 시스템에서 잘못된 결정을 내리는 것은 큰 위험을 가진다.

비용: 현실 세계에서 학습 데이터를 수집하는 과정은 비용이 많이 듬

이러한 문제들을 해결하기 위해 사전에 수집된 데이터셋을 이용해 에이전트가 학습하는 방식인 오프라인 강화학습이 제시되었다.

- <h3> Batch-Constrained Q-learning </h3>
이 알고리즘은 오프라인 강화학습에서 발생하는 "distributional shift" 문제를 해결하기 위해 제안된 알고리즘
이 문제는 학습 과정에서 사용되는 데이터 분포와 실제 테스트 환경에서의 데이터 분포가 다를 때 발생하는데 BCQ는 policy가 데이터셋에서 벗어난 행동을 선택하지 않도록 제약을 가한다

장점: BCQ는 데이터셋 내에서 학습된 행동만을 선택하므로 안정성이 높으며 데이터를 최대한 활용하여 효율적으로 학습할 수 있다.

단점: 데이터셋에 없는 새로운 최적 행동을 발견하기 어려울 수 있으며 여러 구성 요소를 통합해야 하므로 구현이 복잡할 수 있다.

- <h3> Conservative Q-Learning </h3>
과도한 Q-값을 피하기 위해 제안된 알고리즘으로 Q-함수가 지나치게 높은 값을 가지지 않도록 하는 보수적인 정책 평가 방식을 사용

장점: Q-값을 보수적으로 가지기 때문에 policy의 안정성이 높으며 실제 환경에서의 성능이 높아질 가능성이 높다.

단점: 적절한 alphaα 값을 찾기가 힘들며 잘못 설정하면 학습이 비효율적일 수 있다 또한 정규화 항을 계산해야 하므로 계산 비용이 증가할 수 있다.

- <h3> Behavior Regularized Actor-Critic </h3>
오프라인 데이터셋에서 벗어난 행동을 통제하기 위해 정책 정규화를 도입한 알고리즘

장점: 행동 policy가 데이터셋 안의 행동과 유사하도록 유지하여 안정성을 높였으며 다양한 형태의 정규화를 통해 policy를 조절할 수 있다

단점: 적절한 정규화 정도를 찾기 어렵고 과도한 정규화는 학습을 저해할 수 있다

